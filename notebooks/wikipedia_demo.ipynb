{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install kvpress --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50WsX3JqkPjj",
        "outputId": "f982951e-79af-46b1-82ca-6f43db478eb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJ-oFZqj1MA"
      },
      "source": [
        "In this notebook, we showcase how to use the KVpress pipelines by answering questions about NVIDIA Wikipedia article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9LL7CA7gj1MB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "from kvpress import (\n",
        "    ExpectedAttentionPress,\n",
        "    KnormPress,\n",
        "    ObservedAttentionPress,\n",
        "    RandomPress,\n",
        "    SnapKVPress,\n",
        "    StreamingLLMPress\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oMNDeYij1MC"
      },
      "source": [
        "# Load the pipeline and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDyNsELoj1MC",
        "outputId": "819d515f-b6eb-4464-f6bb-2bcd2276c521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Load pipeline\n",
        "\n",
        "device = \"cuda:0\"\n",
        "# ckpt = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "ckpt = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "# ckpt = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# ckpt = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
        "# attn_implementation = \"flash_attention_2\"  # use \"eager\" for ObservedAttentionPress and \"sdpa\" if you can't use \"flash_attention_2\"\n",
        "# use attn_implementation = \"eager\" for ObservedAttentionPress or attn_implementation = \"flash_attention_2\" if you can use flash attention\n",
        "# flash_attention_2 is not fully supported on T4 GPUs, so we are using sdpa\n",
        "attn_implementation = \"sdpa\"\n",
        "pipe = pipeline(\"kv-press-text-generation\", model=ckpt, device=device, torch_dtype=torch.float16, model_kwargs={\"attn_implementation\":attn_implementation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei5uzbjxj1MD",
        "outputId": "8db7a0d7-ef1d-4249-809e-2340f673100e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 10054\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "url = \"https://en.wikipedia.org/wiki/Nvidia\"\n",
        "content = requests.get(url).content\n",
        "soup = BeautifulSoup(content, \"html.parser\")\n",
        "context = \"\".join([p.text for p in soup.find_all(\"p\")]) + \"\\n\\n\"\n",
        "tokens = pipe.tokenizer.encode(context, return_tensors=\"pt\").to(device)\n",
        "print(f\"Number of tokens: {tokens.size(1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXnLsGvlj1MD"
      },
      "source": [
        "# Use the pipeline with a press"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L8Wf-y4hj1MD"
      },
      "outputs": [],
      "source": [
        "# Pick a press with a compression ratio, you can run the following cells with different presses\n",
        "compression_ratio = 0.5\n",
        "press = ExpectedAttentionPress(compression_ratio)\n",
        "# press = KnormPress(compression_ratio)\n",
        "# press = RandomPress(compression_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL2J9vvNj1MD",
        "outputId": "d10e3467-530b-454d-d2a3-ac2149a51cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:   Complete this sentence: The Nvidia GeForce Partner Program was a ...\n",
            "Answer:     marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\n",
            "Prediction: The Nvidia GeForce Partner Program was a program that offered discounts and other benefits to partners who sold Nvidia GPUs to customers. It was designed to help Nvidia expand its market share and increase sales of its graphics cards. The program was open to a wide range of\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline on a single question\n",
        "\n",
        "question = \"Complete this sentence: The Nvidia GeForce Partner Program was a ...\"\n",
        "true_answer = \"marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\"\n",
        "pred_answer = pipe(context, question=question, press=press)[\"answer\"]\n",
        "\n",
        "print(f\"Question:   {question}\")\n",
        "print(f\"Answer:     {true_answer}\")\n",
        "print(f\"Prediction: {pred_answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF2P64Vmj1ME",
        "outputId": "02c80eb7-e396-4d3c-c2a3-92af534d1e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:   What happened on March 1, 2024?\n",
            "Answer:     Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion\n",
            "Prediction: Based on the information provided, there is no specific event or news item for March 1, 2024 mentioned in the given text. The passage covers Nvidia's history and developments from 1993 to 2024\n",
            "\n",
            "Question:   What was the unofficial company motto of Nvidia during the early days?\n",
            "Answer:     Our company is thirty days from going out of business\n",
            "Prediction: According to the passage, the unofficial company motto of Nvidia during the early days was:\n",
            "\n",
            "\"Our company is thirty days from going out of business.\"\n",
            "\n",
            "This quote is attributed to Huang, who began internal presentations to Nvidia staff with those words for many years.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline on multiple questions, the context will be compressed only once\n",
        "\n",
        "questions = [\n",
        "    \"What happened on March 1, 2024?\",\n",
        "    \"What was the unofficial company motto of Nvidia during the early days?\",\n",
        "]\n",
        "\n",
        "true_answers = [\n",
        "    \"Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion\",\n",
        "    \"Our company is thirty days from going out of business\",\n",
        "]\n",
        "\n",
        "pred_answers = pipe(context, questions=questions, press=press)[\"answers\"]\n",
        "for question, pred_answer, true_answer in zip(questions, pred_answers, true_answers):\n",
        "    print(f\"Question:   {question}\")\n",
        "    print(f\"Answer:     {true_answer}\")\n",
        "    print(f\"Prediction: {pred_answer}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvuxa8n4j1ME",
        "outputId": "553055c6-c738-4d33-96e8-cf2a54282112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:              What is GTC ?\n",
            "Answer:                Nvidia's GPU Technology Conference (GTC) is a series of technical conferences held around the world.\n",
            "Prediction w/o prefix: GTC stands for GPU Technology Conference. It is an annual conference and exhibition that focuses on the latest developments and advancements in graphics processing technology, particularly in\n",
            "Prediction w/ prefix : Come on you don't know GTC ? Everyone knows GTC. It's the GPU Technology Conference. It's a major event for the graphics processing unit (GPU) industry. It's where the\n"
          ]
        }
      ],
      "source": [
        "# Use an answer prefix and limit the number of tokens in the answer\n",
        "\n",
        "question = \"What is GTC ?\"\n",
        "true_answer = \"Nvidia's GPU Technology Conference (GTC) is a series of technical conferences held around the world.\"\n",
        "answer_prefix = \"Come on you don't know GTC ? Everyone\"\n",
        "max_new_tokens = 30\n",
        "\n",
        "pred_answer_with_prefix = pipe(context, question=question, answer_prefix=answer_prefix, press=press, max_new_tokens=max_new_tokens)[\"answer\"]\n",
        "pred_answer_without_prefix = pipe(context, question=question, press=press, max_new_tokens=max_new_tokens)[\"answer\"]\n",
        "\n",
        "print(f\"Question:              {question}\")\n",
        "print(f\"Answer:                {true_answer}\")\n",
        "print(f\"Prediction w/o prefix: {pred_answer_without_prefix}\")\n",
        "print(f\"Prediction w/ prefix : {answer_prefix + pred_answer_with_prefix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MsT1pAgj1MF",
        "outputId": "1561242e-935a-4751-ee64-cdbad704ccab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:         Complete this sentence: In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster,\n",
            "Answer:           to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software\n",
            "Prediction w/ Q:  In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster, to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software.\n",
            "Prediction w/o Q: In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster, which was designed to accelerate machine learning and artificial intelligence tasks. The DGX-1 was a high-performance computing system that used Nvidia's\n"
          ]
        }
      ],
      "source": [
        "# SnapKV use the latest queries to prune the KV-cache. It's hence more efficient if we include the question during compression as the latest queries will correspond to the question.\n",
        "# However it implies also implies that SnapKV cannot compress well the context independently of the question (e.g. as in a chat use case)\n",
        "\n",
        "\n",
        "question = \"Complete this sentence: In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster,\"\n",
        "true_answer = (\n",
        "    \"to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software\"\n",
        ")\n",
        "\n",
        "press = SnapKVPress(compression_ratio=0.8)\n",
        "\n",
        "pred_answer_with_question = pipe(context + question, press=press)[\"answer\"]\n",
        "pred_answer_without_question = pipe(context, question=question, press=press)[\"answer\"]\n",
        "\n",
        "print(f\"Question:         {question}\")\n",
        "print(f\"Answer:           {true_answer}\")\n",
        "print(f\"Prediction w/ Q:  {pred_answer_with_question}\")\n",
        "print(f\"Prediction w/o Q: {pred_answer_without_question}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}